[project]
name = "rope-pp"
version = "0.1.0"
description = "RoPE++ Implementation"
requires-python = "==3.12.*"
dependencies = [
    "ninja",
    "packaging",
    "psutil",
    "transformers==4.51.0",
    "datasets",
    "wandb",
    "zstandard",
    "accelerate>=0.26.0",
    "deepspeed",
    "lm-eval",
    "wonderwords",
    "nltk",
]

[project.optional-dependencies]
cpu = [
    "torch==2.8.0",
]
gpu = [
    "torch==2.8.0",
    "flash-attn",
]

[build-system]
requires = ["setuptools>=45", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools]
packages = ["llama_variants", "utils"]

# Target torch to CUDA 12.8 or CPU
[tool.uv.sources]
torch = [
    { index = "pytorch-cpu", extra = "cpu" },
    { index = "pytorch-cu128", extra = "gpu" },
]
flash-attn = [
    { url = "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.5.4/flash_attn-2.8.3+cu128torch2.8-cp312-cp312-linux_x86_64.whl", extra = "gpu" },
]
lm-eval = { git = "https://github.com/EleutherAI/lm-evaluation-harness.git" }

[[tool.uv.index]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu"
explicit = true

[[tool.uv.index]]
name = "pytorch-cu128"
url = "https://download.pytorch.org/whl/cu128"
explicit = true

[tool.uv]
conflicts = [
    [
        { extra = "cpu" },
        { extra = "gpu" },
    ],
]
